# 卷积优化策略
## 滑动窗口法
对卷积计算，我们需要规定如下概念：**特征图**和**卷积核**。

其中**特征图**指代具体输入和输出的张量，可以用来表示传递给卷积算法的原始输入参数，也用来表示输出计算提取的特征张量。而**卷积核**指代一个二维张量（矩阵），用来定义我们应该从输入特征图中提取出什么特征。另外，我们可以认为**卷积核**可以有不止一个，因为有时候（大部分时候）我们可以选择使用多个卷积核来提取输入特征图上的多个特征。同时，**特征图**并不一定是一个二维张量，一般而言我们处理的特征图是个三维（或者可能更高）的张量，这样我们可以使用额外的维度来存储输入的其他信息。举例而言，如果输入的是一张图片，那么三维张量就允许我们存储RGBA四个通道的信息。

为了简化表述，下文提到的所有特征图，除特殊声明外，维度按三维计算，也就是 $[C, H, W]$。其中 $C$ 是Channel，通道，而 $H,W$则是张量的宽和高。举例说明，假设我们在处理一张图片，那么Channel就可能包含`RGBW`这些信息，而 $H,W$就是图片的高和宽。下面我们会给出关于上面提到概念的形式化定义以便下文继续深入讨论。我们定义：
- **输入特征图**定义为 $T_\text{in}$，其三个维度分别为 $[C_\text{in}, H_\text{in}, W_\text{in}]$
- **输出特征图**定义为 $T_\text{out}$，其三个维度分别为 $[C_\text{out}, H_\text{out}, W_\text{out}]$
- **卷积核**定义为 $T_\text{kernel}$，其宽和高分别为 $[H_\text{kernel}, W_\text{kernel}]$，卷积核的数量为 $n_k$
- 卷积过程中的**步长**和**填充**分别为 $S$和 $P$

我们来看一下原始的卷积算法，也就是滑动窗口法的定义。假设 $P=0,S=1$，则原始的卷积算法可以被定义为如下公式：

$$
\large{T_\text{out}[i,j] = \sum^{W_\text{kernel}}_{n=0}{\sum^{H_\text{kernel}}_{k=0}{T_\text{kernel}[n,k] \times T_\text{in}[i+n, j+k]}}}
$$

如果我们考虑多通道的情况，同时忽略步长和填充的特值，则上面的公式可以进一步被拓展为：

$$
\large{T_{\text{out}}[c_{\text{out}}, i, j] = \sum_{c_{\text{in}}=0}^{C_{\text{in}}-1} \sum_{n=0}^{H_{\text{kernel}}-1} \sum_{k=0}^{W_{\text{kernel}}-1} T_{\text{kernel}}[c_{\text{out}}, c_{\text{in}}, n, k] \times T_{\text{in}}[c_{\text{in}}, i \times S + n - P, j \times S + k - P]}
$$

可以看到这里我们采用循环滑动窗口的方法进行计算的效率是相对比较低的，并且由于需要频繁存取非连续内存，其缓存命中率在较大的输入特征图上也会大幅降低。为了解决这个问题，我们需要采用img2col和winograd两种算法来优化。

## img2col算法
现在的主流硬件和算法普遍都对矩阵乘计算有较好的性能优化，所以如果我们希望提升卷积算法的效率，将其通过某种方式转为矩阵乘计算就是可行方案之一。

首先我们需要引入**感受野**这个概念。所谓感受野实际上就是卷积核 $T_\text{kernel}$在输入特征图 $T_\text{in}$上覆盖的区域。为方便理解，实际上所谓的感受野可以被简单理解为把卷积核“放在”输入特征图上，它盖住的区域我们就称为是这个卷积核的感受野。而在一张输入特征图上，我们会发现总的感受野数量是恒定的，其数量与卷积核的大小、输入特征图的大小、填充和步长均有关。我们也很容易想到，感受野的数量也可以通过前述四个变量唯一确定，但这里我们先获取一个对感受野数量的直观认知，其规范定义在下文中再行定义。

随后我们来考虑一个最基础的情况，也就是当卷积核的尺寸和输入特征图均为 $[W,H]$，并且两者刚好完全覆盖的时候我们需要进行的计算。很明显，这里我们需要计算的是对输入特征图上感受野内每一点和卷积核上对应的每一点的乘积之和，并将其赋值给输出特征图上的一点 $C$，也即：

$$
\large{C = \sum^{W}_{n=0}{\sum^{H}_{k=0}{T_\text{kernel}[n,k] \times T_\text{in}[n, k]}}}
$$

进一步地，我们注意到这里的二维矩阵实际上可以被展平为一维向量。我们以行优先存储为例（列优先也是一样的，因为内外层求和之间没有额外操作，交换求和顺序不改变求和结果），当我们将输入特征图和卷积核两者展开后，这两者会变成长度为 $[W \times H]$ 的向量 $V_\text{kernel}$和 $V_\text{in}$。此时我们可以将上文的公式进一步地变形为：

$$
\large{C = \sum^{W \times H}_{n=0}{V_\text{kernel}[n] \times V_\text{in}[n]}}
$$

我们注意到上面这个变形后的公式和向量点积计算的公式没有区别，因此我们可以将它进一步地变形为两个向量的点积，如下：

$$\large{C = V_\text{in}^T \cdot V_\text{kernel}}$$

这样，我们就将一次基础的卷积运算化为了向量点积。而结合上文提到的原始卷积算法，不难注意到卷积计算本身就是在对输入特征图上每一个可能且不重复的感受野执行上述的基础卷积运算，而我们可以进一步地把上面的向量点积扩展到多次计算上。

在讨论之前，我们需要做一个额外定义。由于不位于零点的感受野展开表示会比较冗长，读起来会很麻烦，因此在这里我们需要做一个小小的变换来让整体计算看起来更清晰：我们将用不同的感受野来代替上文提到的滑动窗口，因为这两者本质上是等价的。由于总的感受野数量是恒定的，我们也可以进一步定义感受野。我们定义感受野总集为  $\mathbb{F}$，则感受野可以表示为 $F_n \in \mathbb{F}$，感受野的数量也可以表示为 $|\mathbb{F}|$。同时，我们额外规定感受野是按上文滑动窗口顺序扫描得到的，以免下文产生歧义。

接下来我们对上面的点积计算进行拓展。我们仍然先考虑和上文相同的情况，也就是输入特征图的尺寸和卷积核的尺寸一致，只是现在我们考虑一个标准的点积计算，那么上面的向量点积实际上也可以被作为矩阵乘来对待。在这里，两个矩阵的尺寸均为 $[W \times H，1]$，上文提到的点积计算可以被重新表示如下：

$$\large{ \overset{[1,W \times H]}{\left(\overset{[W \times H, 1]}{V_\text{in}}\right)^T} \cdot \overset{[W \times H, 1]}{V_\text{kernel}} = \overset{[1,1]}{V_\text{out}}}$$

如果我们有多个感受野 $\{F_0, \cdots F_n\}\in \mathbb{F}$，那么我们当然可以按上面的公式执行 $n$轮计算。进一步地，我们会发现，无论感受野的数量是多少，展开后的 $V_\text{in}$和 $V_\text{kernel}$的第一个维度大小总是 $[W \times H]$。这样，上述的矩阵乘公式就可以被拓展如下：

$$\large{V_{\text{out}}[i] = \overset{[1,W \times H]}{\left(\overset{[W \times H, 1]}{V_{\text{in},i}}\right)^T} \cdot \overset{[W \times H, 1]}{V_{\text{kernel}}} \quad \text{for} \, i = 0, 1, \ldots, |\mathbb{F}|-1}$$

我们注意到，上面的计算实质上等价于将所有的 $V_\text{in}^T$连接起来构成一个矩阵，再将这个矩阵直接乘以 $V_\text{kernel}$，这样得到的值恰好就等价于我们执行上述计算再给 $V_\text{out}[i]$赋值的结果。因此，我们可以进一步地把上式简化为：

$$\large{
\overset{[|\mathbb{F}|,W \times H]}{\begin{bmatrix}
V_{\text{in},0} \\
\vdots \\
V_{\text{in},|\mathbb{F}|}
\end{bmatrix}^T}}\cdot \overset{[W \times H, 1]}{V_\text{kernel}} = \overset{[|\mathbb{F}|,1]}{V_\text{out}}
$$

我们再回顾一下 $V_{\text{in}}$的来历，它是由输入特征图上的感受野展开成行向量得到的。既然我们现在使用 $V_{\text{in}}$都需要将其转置后再使用，那么我们不如直接在最开始展开的时候就改为展开成列向量 $C_{\text{in}}$，这样也就省去了转置的步骤，变为：

$$\large{
\overset{[|\mathbb{F}|,W \times H]}{\begin{bmatrix}
C_{\text{in},0}, \cdots , C_{\text{in},|\mathbb{F}|}
\end{bmatrix}}}\cdot \overset{[W \times H, 1]}{V_\text{kernel}} = \overset{[|\mathbb{F}|,1]}{V_\text{out}}
$$

接下来我们看一下之前一直被搁置不谈的感受野数量 $|\mathbb{F}|$。首先回顾一下，我们对 $\mathbb{F}$的定义是**输入特征图上每一个可能且不重复的感受野的总集**，而这从本质上说可以理解为原始算法里滑动窗口滑动的总次数。还是和上文一样，我们先用 $P=1,S=1,H_\text{kernel}=3, W_\text{kernel}=3$这个特殊条件来看一下最基础的公式。在这个特殊条件下，我们可以认为输出特征图的大小与输入特征图的大小是完全一致的，因为每次滑动都会移动一格的位置，而只看其中心点的话正好扫过了完整的输入特征图。这样一来，我们就可以将感受野数量定义为：

$$|\mathbb{F}| = H_\text{in} \times W_\text{in}$$

这个定义很简单，但是也很粗糙。

我们接下来考虑不约束 $P$的情况。 $H_\text{out}$会变成 $H_\text{in} + 2P - 2$，其中 $-2$是因为我们需要减去卷积核本身能跨越的宽度，所以需要让减去卷积核本身的宽度减一。 $W_\text{in}$也是同理。

然后考虑不约束 $S$的情况。在这里我们需要额外留意，卷积核本身的宽度减一这个计算应当发生在整体步长计算之外，因为我们希望按整体步长计算完之后再加上这个调整值。此时， $H_\text{out}$会变成 $\large{\frac{H_\text{in} + 2P - 3}{S}}+1$

最后我们去掉对卷积核尺寸的限制。其实上面的公式已经相当接近了，在进行一次简单的代换后我们就能得到：

$$
|\mathbb{F}| = H_\text{out} \times W_\text{out},\ \text{where}
\begin{cases}
H_\text{out} = \large{\frac{H_\text{in} + 2P - H_\text{kernel}}{S}}+1 \\[0.2cm]
W_\text{out} = \large{\frac{W_\text{in} + 2P - W_\text{kernel}}{S}}+1 \\
\end{cases}
$$

这样还存在最后一个问题。让我们回顾一下原始的滑动窗口法，其获取到的 $T_\text{out}$的形状应当与 $T_\text{in}$成某种比例关系，而不是像现在获取的 $V_\text{out}$一样是一根一维的向量。为了解决这个问题，我们需要将 $T_\text{out}$重新展开到我们预期的形状。这一步所需的信息其实在上面刚好已经拿到了，我们只需要将拿到的结果向量根据滑动窗口（因为img2col可以理解为是用另一种方式表示的滑动窗口）的顺序方向套用上文计算出的 $H_\text{out}$ 和 $W_\text{out}$ 即可。具体可以用如下公式进行计算：

$$T_\text{out}[i,j] = W_\text{out} \times j + i$$

