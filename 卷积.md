# 卷积优化策略
## 滑动窗口法
对卷积计算，我们需要规定如下概念：**特征图**和**卷积核**。

其中**特征图**指代具体输入和输出的张量，可以用来表示传递给卷积算法的原始输入参数，也用来表示输出计算提取的特征张量。而**卷积核**指代一个二维张量（矩阵），用来定义我们应该从输入特征图中提取出什么特征。另外，我们可以认为**卷积核**可以有不止一个，因为有时候（大部分时候）我们可以选择使用多个卷积核来提取输入特征图上的多个特征。同时，**特征图**并不一定是一个二维张量，一般而言我们处理的特征图是个三维（或者可能更高）的张量，这样我们可以使用额外的维度来存储输入的其他信息。举例而言，如果输入的是一张图片，那么三维张量就允许我们存储RGBA四个通道的信息。

为了简化表述，下文提到的所有特征图，除特殊声明外，维度按三维计算，也就是$[C, H, W]$。其中$C$是Channel，通道，而$H,W$则是张量的宽和高。举例说明，假设我们在处理一张图片，那么Channel就可能包含`RGBW`这些信息，而$H,W$就是图片的高和宽。下面我们会给出关于上面提到概念的形式化定义以便下文继续深入讨论。我们定义：
- **输入特征图**定义为$T_\text{in}$，其三个维度分别为$[C_\text{in}, H_\text{in}, W_\text{in}]$
- **输出特征图**定义为$T_\text{out}$，其三个维度分别为$[C_\text{out}, H_\text{out}, W_\text{out}]$
- **卷积核**定义为$T_\text{kernel}$，其宽和高分别为$[H_\text{kernel}, W_\text{kernel}]$，卷积核的数量为$n_k$
- 卷积过程中的**步长**和**填充**分别为$S$和$P$

我们来看一下原始的卷积算法，也就是滑动窗口法的定义。假设$P=0,S=1$，则原始的卷积算法可以被定义为如下公式：
$$\large{T_\text{out}[i,j] = \sum^{W_\text{kernel}}_{n=0}{\sum^{H_\text{kernel}}_{k=0}{T_\text{kernel}[n,k] \times T_\text{in}[i+n, j+k]}}}$$
如果我们考虑多通道的情况，同时忽略步长和填充的特值，则上面的公式可以进一步被拓展为：
$$\large{T_{\text{out}}[c_{\text{out}}, i, j] = \sum_{c_{\text{in}}=0}^{C_{\text{in}}-1} \sum_{n=0}^{H_{\text{kernel}}-1} \sum_{k=0}^{W_{\text{kernel}}-1} T_{\text{kernel}}[c_{\text{out}}, c_{\text{in}}, n, k] \times T_{\text{in}}[c_{\text{in}}, i \times S + n - P, j \times S + k - P]}$$
可以看到这里我们采用循环滑动窗口的方法进行计算的效率是相对比较低的，并且由于需要频繁存取非连续内存，其缓存命中率在较大的输入特征图上也会大幅降低。为了解决这个问题，我们需要采用img2col和winograd两种算法来优化。

## img2col算法
现在的主流硬件和算法普遍都对矩阵乘计算有较好的性能优化，所以如果我们希望提升卷积算法的效率，将其通过某种方式转为矩阵乘计算就是可行方案之一。

首先我们需要引入**感受野**这个概念。所谓感受野实际上就是卷积核$T_\text{kernel}$在输入特征图$T_\text{in}$上覆盖的区域。为方便理解，实际上所谓的感受野可以被简单理解为把卷积核“放在”输入特征图上，它盖住的区域我们就称为是这个卷积核的感受野。而在一张输入特征图上，我们会发现总的感受野数量是恒定的，其数量与卷积核的大小、输入特征图的大小、填充和步长均有关。我们也很容易想到，感受野的数量也可以通过前述四个变量唯一确定，但这里我们先获取一个对感受野的直观认知，其规范定义在下文中再行定义。

随后我们来考虑一个最基础的情况，也就是当卷积核的尺寸和输入特征图均为$[W,H]$，并且两者刚好完全覆盖的时候我们需要进行的计算。很明显，这里我们需要计算的是对输入特征图上感受野内每一点和卷积核上对应的每一点的乘积之和，并将其赋值给输出特征图上的一点$C$，也即：

$$\large{C = \sum^{W}_{n=0}{\sum^{H}_{k=0}{T_\text{kernel}[n,k] \times T_\text{in}[n, k]}}}$$

进一步地，我们注意到这里的二维矩阵实际上可以被展平为一维向量。我们以行优先存储为例（列优先也是一样的，因为内外层求和之间没有额外操作，交换求和顺序不改变求和结果），当我们将输入特征图和卷积核两者展开后，这两者会变成长度为$[W \times H]$ 的向量$V_\text{kernel}$和$V_\text{in}$。此时我们可以将上文的公式进一步地变形为：

$$\large{C = \sum^{W \times H}_{n=0}{V_\text{kernel}[n] \times V_\text{in}[n]}}$$

我们注意到上面这个变形后的公式和向量点积计算的公式没有区别，因此我们可以将它进一步地变形为两个向量的点积，如下：

$$\large{C = V_\text{in}^T \cdot V_\text{kernel}}$$

这样，我们就将一次基础的卷积运算化为了向量点积。而结合上文提到的原始卷积算法，不难注意到卷积计算本身就是在对输入特征图上每一个可能且不重复的感受野执行上述的基础卷积运算，而我们可以进一步地把上面的向量点积扩展到多次计算上。

在讨论之前，我们需要做一个额外定义。由于不位于零点的感受野展开表示会比较冗长，读起来会很麻烦，因此在这里我们需要做一个小小的变换来让整体计算看起来更清晰：我们将用不同的感受野来代替上文提到的滑动窗口，因为这两者本质上是等价的。由于总的感受野数量是恒定的，我们也可以进一步定义感受野。我们定义感受野总集为$\mathbb{F}$，则感受野可以表示为$F_n \in \mathbb{F}$，感受野的数量也可以表示为$|\mathbb{F}|$。同时，我们额外规定感受野是按上文滑动窗口顺序扫描得到的，以免下文产生歧义。

接下来我们对上面的点积计算进行拓展。我们仍然先考虑和上文相同的情况，也就是输入特征图的尺寸和卷积核的尺寸一致，只是现在我们考虑一个标准的点积计算，那么上面的向量点积实际上也可以被作为矩阵乘来对待。在这里，两个矩阵的尺寸均为$[W \times H，1]$，上文提到的点积计算可以被重新表示如下：

$$\large{ \overset{[1,W \times H]}{\left(\overset{[W \times H, 1]}{V_\text{in}}\right)^T} \cdot \overset{[W \times H, 1]}{V_\text{kernel}} = \overset{[1,1]}{V_\text{out}}}$$

如果我们有多个感受野$\{F_0, \cdots F_n\}\in \mathbb{F}$，那么
